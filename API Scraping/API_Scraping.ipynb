{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Задание 1.\n",
    "Обязательная часть\n",
    "Будем парсить страницу со свежеми новостям на habr.com/ru/all/.\n",
    "\n",
    "Вам необходимо собирать только те статьи, в которых встречается хотя бы одно требуемое ключевое слово. Эти слова определяем в начале кода в переменной, например:\n",
    "\n",
    "KEYWORDS = ['python', 'парсинг']\n",
    "\n",
    "Поиск вести по всей доступной preview-информации (это информация, доступная непосредственно с текущей страницы).\n",
    "\n",
    "В итоге должен формироваться датафрейм вида: <дата> - <заголовок> - <ссылка>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date                                              title  \\\n",
      "0  2020-12-06    Власти Беларуси опять пытаются глушить Telegram   \n",
      "0  2020-12-05  Spotify научила модель на основе ИИ искать пла...   \n",
      "0  2020-12-04  Постаматы PickPoint стали автоматически открыв...   \n",
      "0  2020-12-04  Правообладатели обвинили Яндекс.Музыку в невып...   \n",
      "\n",
      "                                 link  \\\n",
      "0  https://habr.com/ru/news/t/531522/   \n",
      "0  https://habr.com/ru/news/t/531424/   \n",
      "0  https://habr.com/ru/news/t/531414/   \n",
      "0  https://habr.com/ru/news/t/531362/   \n",
      "\n",
      "                                                text  \n",
      "0  Сегодня примерно в 11:35 все операторы связи Б...  \n",
      "0  \\n\\r\\nSpotify подала пантентную заявку на моде...  \n",
      "0  В работе сервиса PickPoint произошел сбой из-з...  \n",
      "0  Правообладатели обвинили сервис Яндекс.Музыка ...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime, date, timedelta\n",
    "import dateparser\n",
    "\n",
    "def convert_date(date_txt):\n",
    "    dt = None\n",
    "    if date_txt == \"вчера\":\n",
    "        today = date.today()\n",
    "        dt = today - timedelta(days=1)\n",
    "    elif date_txt == \"сегодня\":\n",
    "        dt = date.today()\n",
    "    else:\n",
    "        dt = dateparser.parse(date_txt).date()\n",
    "    return dt\n",
    "\n",
    "\n",
    "def get_links(url, start_from_date):\n",
    "    start_from_date = datetime.strptime(start_from_date, '%Y-%m-%d').date()\n",
    "    page = 1\n",
    "    all_links = []\n",
    "    while True:\n",
    "        prfx = \"\" if page == 1 else f\"page{page}\"\n",
    "        news = requests.get(f\"{url}{prfx}\")\n",
    "        if news.status_code == 404:\n",
    "            break\n",
    "        soup = BeautifulSoup(news.text, 'html.parser')\n",
    "        articals = soup.find_all('article', class_='post post_preview')\n",
    "        for artical in articals:\n",
    "            date_text = artical.find('span', class_=\"post__time\").text.split(\" в \")[0].strip()\n",
    "            dt = convert_date(date_text)\n",
    "            if dt >= start_from_date:\n",
    "                link = artical.find('a', class_='post__title_link').get(\"href\")\n",
    "                all_links.append(link)\n",
    "            else:\n",
    "                return all_links\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(0.3)\n",
    "    return\n",
    "\n",
    "\n",
    "KEYWORDS = ['музыка', 'VK']\n",
    "base_url = \"https://habr.com/ru/news/\"\n",
    "all_links = get_links(base_url, \"2020-12-03\")\n",
    "kom_news = pd.DataFrame()\n",
    "\n",
    "for link in all_links[:len(all_links)]:  # ибо долго\n",
    "    soup = BeautifulSoup(requests.get(link).text, 'html.parser')\n",
    "    time.sleep(0.3)\n",
    "    date = pd.to_datetime(soup.find(class_='post__time').get('data-time_published'), yearfirst=True).date()\n",
    "    title = soup.find(class_='post__title-text').text\n",
    "    text = soup.find(id='post-content-body').text\n",
    "    if any([re.search(kwd, text) for kwd in KEYWORDS]):\n",
    "        row = {'date': date, 'title': title, \"link\": link, 'text': text}\n",
    "        kom_news = pd.concat([kom_news, pd.DataFrame([row])])\n",
    "\n",
    "print(kom_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 2.\n",
    "Написать скрипт, который будет проверять список e-mail адресов на утечку при помощи сервиса Avast Hack Ckeck. Список email-ов задаем переменной в начале кода:\n",
    "EMAIL = [xxx@x.ru, yyy@y.com]\n",
    "\n",
    "В итоге должен формироваться датафрейм со столбцами: <почта> - <дата утечки> - <источник утечки> - <описание утечки>\n",
    "\n",
    "Подсказка: сервис работает при помощи \"скрытого\" API. Внимательно изучите post-запросы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             email                    date                   site  \\\n",
      "0    bro@gmail.com               July 2019        cprewritten.net   \n",
      "82   bro@gmail.com           December 2012    heroesofnewerth.com   \n",
      "81   bro@gmail.com            October 2018               pluto.tv   \n",
      "79   bro@gmail.com                May 2017             edmodo.com   \n",
      "78   bro@gmail.com                May 2020           homechef.com   \n",
      "..             ...                     ...                    ...   \n",
      "58  nick@gmail.com           November 2020  iquiquepropiedades.cl   \n",
      "90  nick@gmail.com           November 2020      frankkoretsky.com   \n",
      "38  nick@gmail.com           December 2018          storybird.com   \n",
      "36  nick@gmail.com           November 2020           iitbrain.com   \n",
      "25  nick@gmail.com  At an unconfirmed date          trackmill.com   \n",
      "\n",
      "                                          description  \n",
      "0   In July 2019, virtual world Club Penguin Rewri...  \n",
      "82  In December 2012, the online MMOG Heroes of Ne...  \n",
      "81  In October 2018, the American internet televis...  \n",
      "79  In May 2017, Edmodo's user database was breach...  \n",
      "78  In May 2020, the hacking group “ShinyHunters” ...  \n",
      "..                                                ...  \n",
      "58  In November 2020, a collection of over 23,000 ...  \n",
      "90  In November 2020, a collection of over 23,000 ...  \n",
      "38  In December 2018, Storybird's database was all...  \n",
      "36  In November 2020, a collection of over 23,000 ...  \n",
      "25  At an unconfirmed date, the online gaming webs...  \n",
      "\n",
      "[119 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class MailChecker:\n",
    "    emailAddresses: List[str]\n",
    "df = pd.DataFrame()\n",
    "EMAILS = ['bro@gmail.com', 'nick@gmail.com']\n",
    "url = \"https://identityprotection.avast.com/v1/web/query/site-breaches/unauthorized-data\"\n",
    "headers = {\n",
    "\"Access-Control-Expose-Headers\": \"Vaar-Version, Vaar-Status\",\n",
    "\"Vaar-Header-App-Product\": \"hackcheck-web-avast\",\n",
    "\"Vaar-Version\": \"0\",\n",
    "\"Vaar-Status\": \"0\",\n",
    "}\n",
    "data = asdict(MailChecker(EMAILS))\n",
    "s = requests.Session()\n",
    "resp = s.post(url, json=data, headers=headers)\n",
    "\n",
    "breaches = resp.json()[\"breaches\"]\n",
    "summary = resp.json()[\"summary\"]\n",
    "\n",
    "def filtr_email(row, summary):\n",
    "    for k,v in summary.items():\n",
    "        if row['breachId'] in v[\"breaches\"]:\n",
    "            return k\n",
    "\n",
    "branch_df = pd.DataFrame(breaches).transpose()\n",
    "branch_df[\"email\"] = branch_df.apply(filtr_email, args=(summary,), axis=1 )\n",
    "branch_df[\"date\"] = branch_df[\"description\"].apply(lambda x: x.split(\",\")[0].replace(\"In \",\"\"))\n",
    "branch_df = branch_df.reset_index()[[\"email\", \"date\",\"site\",\"description\"]].sort_values(\"email\")\n",
    "\n",
    "print(branch_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
